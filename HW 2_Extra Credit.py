# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SyirHafVRbjdACXqBuCcMumXG9C7Zyxg
"""

# Step 1: Imports
import numpy as np
import pandas as pd
import yfinance as yf
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
import pytorch_lightning as pl
import os

# Step 2: Download and normalize input/target
def get_price(tick, start='2020-01-01', end=None):
    return yf.Ticker(tick).history(start=start, end=end)['Close']

def get_prices(tickers, start='2020-01-01', end=None):
    df = pd.DataFrame()
    for s in tickers:
        df[s] = get_price(s, start, end)
    return df

feature_stocks = ['tsla','meta','nvda','amzn','nflx','gbtc','gdx','intc','dal','c',
                  'goog','aapl','msft','ibm','hp','orcl','sap','crm','hubs','twlo']
predict_stock = 'msft'

allX = get_prices(feature_stocks, start='2020-01-01')
ally = get_prices([predict_stock], start='2020-01-01')

# Normalize input
allX = (allX - allX.mean()) / allX.std()

# Normalize target
target_mean = ally.mean().values[0]
target_std = ally.std().values[0]
ally_norm = (ally - target_mean) / target_std
y_mean = torch.tensor(target_mean, dtype=torch.float32)
y_std = torch.tensor(target_std, dtype=torch.float32)

# Step 3: Dataset
class StockDataset(Dataset):
    def __init__(self, X, Y, days):
        self.X = X
        self.Y = Y.reshape(-1)
        self.days = days

    def __len__(self):
        return len(self.Y) - self.days

    def __getitem__(self, index):
        x = self.X[:, index:index+self.days]
        y = self.Y[index + self.days]
        return x, y

DAYS = 32
dataset = StockDataset(allX.to_numpy().T.astype(np.float32),
                       ally_norm.to_numpy().astype(np.float32),
                       days=DAYS)

# Step 4: DataModule
class StockDataModule(pl.LightningDataModule):
    def __init__(self, dataset, batch_size):
        super().__init__()
        self.dataset = dataset
        self.batch_size = batch_size

    def setup(self, stage=None):
        total = len(self.dataset)
        train_size = int(0.7 * total)
        val_size = int(0.15 * total)
        test_size = total - train_size - val_size
        self.train_set, self.val_set, self.test_set = random_split(
            self.dataset, [train_size, val_size, test_size],
            generator=torch.Generator().manual_seed(42)
        )

    def train_dataloader(self):
        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)

    def val_dataloader(self):
        return DataLoader(self.val_set, batch_size=self.batch_size)

    def test_dataloader(self):
        return DataLoader(self.test_set, batch_size=self.batch_size)

# Step 5: Final CNN with Residual Block and Tanh Output
class FinalCNN(pl.LightningModule):
    def __init__(self, dropout_rate=0.05, learning_rate=3e-4):
        super().__init__()
        self.save_hyperparameters()

        self.conv1 = nn.Conv1d(20, 32, kernel_size=5, padding=2)
        self.bn1 = nn.BatchNorm1d(32)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm1d(64)
        self.conv3 = nn.Conv1d(64, 32, kernel_size=1)
        self.bn3 = nn.BatchNorm1d(32)
        self.residual = nn.Conv1d(32, 32, kernel_size=1)

        self.dropout = nn.Dropout(self.hparams.dropout_rate)
        self.gap = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Linear(32, 1)
        self.tanh = nn.Tanh()

        self.criterion = nn.SmoothL1Loss()

    def forward(self, x):
        x = x.permute(0, 1, 2)
        x1 = torch.nn.functional.gelu(self.bn1(self.conv1(x)))
        x2 = torch.nn.functional.gelu(self.bn2(self.conv2(x1)))
        x3 = torch.nn.functional.gelu(self.bn3(self.conv3(x2)))
        x_res = self.residual(x1) + x3
        x = self.dropout(x_res)
        x = self.gap(x).squeeze(-1)
        return self.fc(x).squeeze()

    def training_step(self, batch, batch_idx):
        x, y = batch
        loss = self.criterion(self(x), y)
        self.log("train_mse", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        loss = self.criterion(self(x), y)
        self.log("val_mse", loss)
        return loss

    def test_step(self, batch, batch_idx):
        x, y = batch
        preds = self(x)
        preds_real = preds * y_std + y_mean
        y_real = y * y_std + y_mean
        loss = nn.MSELoss()(preds_real, y_real)
        self.log("test_mse", loss)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate, weight_decay=1e-4)
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.5)
        return {"optimizer": optimizer, "lr_scheduler": scheduler}

# Step 6: Train and Test
model = FinalCNN(dropout_rate=0.05, learning_rate=1e-4)
datamodule = StockDataModule(dataset, batch_size=32)

trainer = pl.Trainer(max_epochs=400, logger=True)
trainer.fit(model, datamodule=datamodule)
trainer.test(model, datamodule=datamodule)

# Step 7: Save model
os.makedirs("saved_models", exist_ok=True)
torch.save(model.state_dict(), "saved_models/final_cnn_real_mse.pth")
print("Model saved to saved_models/final_cnn_real_mse.pth")

# Step 8: Parameter comparison
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

final_param_count = count_parameters(model)
baseline_param_count = 63521  # Original model parameter count

print(f"Baseline Model Parameters: {baseline_param_count}")
print(f"Your FinalCNN Model Parameters: {final_param_count}")